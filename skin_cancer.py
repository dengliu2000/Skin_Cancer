# -*- coding: utf-8 -*-
"""Skin_Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rNio45DK8grbzyvIYbgZzPgu6CtZdTSw

## 資料來源
Skin Cancer MNIST: HAM10000 https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000

### 資料準備
"""

#keras.utils: 做one-hot encoding用
#sklearn.model_selection: 分割訓練集和測試集
#os: 用來建立檔案、刪除檔案
#PIL: (圖像處理庫)匯入圖像
#seed: 設定種子，使每次隨機產生的資料有相同結果。可將數字改成自己的學號(或其他數字)
import numpy as np
import pandas as pd
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
import os
from PIL import Image
np.random.seed(409570406)

#7項皮膚疾病簡稱與全名
lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'Melanoma',
    'bkl': 'Benign keratosis-like lesions ',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

pd.Categorical(lesion_type_dict).codes

!pip uninstall gdown -y && pip install gdown
!gdown -V

# Download from Google Drive
import gdown
url = 'https://drive.google.com/uc?id=1kklF0GDZ-4Vh52MIdTexky6Bqzek7S-c'
output = 'project03.zip'
gdown.download(url, output, quiet=False)

!unzip project03.zip

#讀取影像資料，28*28*3個像素值欄位(pixel0000-pixel2351) + 1個分類類別欄位label
load_img = pd.read_csv('project3_train.csv')

#列出data的標籤
load_img.head()

#檢查讀取圖片的大小與數量
load_img.shape

load_img.iloc[: , :-1].values

#iloc選取特定範圍，讀取種類編號
X_img , y_label = load_img.iloc[: , :-1].values , load_img.iloc[: , -1].values

#將串列轉成矩陣
X_img_train = np.asarray(X_img.tolist())

#將一維的數據，轉換成三維(長*寬*RGB三色)
X_img_train=X_img_train.reshape(X_img_train.shape[0],28,28,3)

#檢查學習資料的照片數量、尺寸大小、維度
print("train data:",'images:',X_img_train.shape," labels:",y_label.shape)

#標準化: 同除255(因為image的數字是0~255)
X_img_train_normalize = X_img_train.astype('float32') / 255.0

#使用np_utils.to_categorical()傳入各參數的label標籤欄位，再執行OneHot encoding (轉成0或1的組合)
y_label_train_OneHot = np_utils.to_categorical(y_label)

#檢查標籤總共有多少種分類
#這裡是共8008筆資料，每筆是7個0或1的組合
y_label_train_OneHot.shape

"""### 建立與訓練CNN模型"""

#匯入keras中的Sequential、layers模組(Dense、 Dropout、 Activation、 Flatten、Conv2D、 MaxPooling2D、 ZeroPadding2D)
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D

# Design your CNN model\

#建立keras的Sequential模型
model_cnn = Sequential()

model_cnn.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(28,28,3),
                     activation='relu',padding='same'))

model_cnn.add(Conv2D(filters=32,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(Conv2D(filters=32,kernel_size=(3,3),
                     activation='relu',padding='same'))

model_cnn.add(MaxPooling2D(pool_size=(2,2)))



model_cnn.add(Conv2D(filters=64,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(Conv2D(filters=64,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(MaxPooling2D(pool_size=(2,2)))


model_cnn.add(Conv2D(filters=128,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(Conv2D(filters=128,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(MaxPooling2D(pool_size=(2,2)))


model_cnn.add(Conv2D(filters=256,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(Conv2D(filters=256,kernel_size=(3,3),
                     activation='relu',padding='same'))
model_cnn.add(MaxPooling2D(pool_size=(2,2)))



model_cnn.add(Flatten())
model_cnn.add(Dense(64,activation='relu'))
model_cnn.add(Dropout(0.3))
model_cnn.add(Dense(7,activation='softmax'))

print(model_cnn.summary())

from keras.callbacks import EarlyStopping, ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss',patience=20, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=10, verbose=1, min_lr=1e-6)

model_cnn.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

epochs=150
batch_size=128

x_train, x_validate, y_train, y_validate = train_test_split(X_img_train_normalize, y_label_train_OneHot, test_size=0.2, shuffle=True)

train_history=model_cnn.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_validate,y_validate), epochs=epochs, verbose=1, callbacks=None)#callbacks=[early_stopping,reduce_lr]

import matplotlib.pyplot as plt
def show_train_history(train_acc,test_acc, yAxisName):
  plt.plot(train_history.history[train_acc])
  plt.plot(train_history.history[test_acc])
  plt.title('Train History')
  plt.ylabel(yAxisName)
  plt.xlabel('Epoch')
  plt.legend(['train','val'], loc='upper left')
  plt.show

show_train_history('accuracy','val_accuracy','Accuracy')

show_train_history('loss','val_loss','Loss')

scores = model_cnn.evaluate(x_validate, y_validate, verbose=0)
scores[1]

prediction = np.argmax(model_cnn.predict(x_validate), axis=-1)
pd.crosstab(np.argmax(y_validate, axis=-1), prediction, rownames=['label'], colnames=['predict'])

# 使用最後的模型進行測試資料預測
load_test_img = pd.read_csv('project3_test.csv')
img_test = load_test_img.values

x_test=img_test.reshape(img_test.shape[0],28,28,3)
x_test_normalize = x_test.astype('float32') / 255.0

df_submit = pd.DataFrame([], columns=['Id', 'Label'])
df_submit['Id'] = [f'{i:04d}' for i in range(len(x_test_normalize))]
df_submit['Label'] = np.argmax(model_cnn.predict(x_test_normalize), axis=-1)

df_submit.to_csv('submission_CNN7.csv', index=None)

"""# CNN"""

# 切割訓練集資料
x_img_train, x_img_test, y_label_train, y_label_test = train_test_split(X_img_train_normalize, y_label_train_OneHot, test_size=0.2, random_state=2)

# 將訓練集資料標準化
x_img_train_normalize = x_img_train.astype('float32') / 255.0
x_img_test_normalize = x_img_test.astype('float32') / 255.0

# 轉換分類標籤成one-hot encoding
y_label_train_OneHot = y_label_train
y_label_test_OneHot = y_label_test

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D  #捲機、池化

# 搭建CNN模型的第一層
model = Sequential()
model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(28,28,3),activation='relu', padding='same'))#激活函數

# 設計卷積層與池化層、平坦層、全連接層

model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3, 3),activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(filters=128, kernel_size=(3, 3),activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))


model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.25)) #隨機丟棄

# 最後一層為輸出層
model.add(Dense(7, activation='softmax'))

model.summary()

# 打包神經網路模型
model.compile(loss='categorical_crossentropy',
       optimizer='adam', metrics=['categorical_accuracy'])

# 訓練網路模型
# 開始訓練 (確認已設定使用GPU的運算資源)
train_history=model.fit(x_img_train_normalize, y_label_train_OneHot,
                        validation_split=0.2,
                        epochs=5, batch_size=4, verbose=1)

# 使用最後的模型進行測試資料預測
load_test_img = pd.read_csv('project3_test.csv')
img_test = load_test_img.values

# 將要測試的資料進行前處理
x_test=img_test.reshape(img_test.shape[0],28,28,3)
x_test_normalize = x_test.astype('float32') / 255.0

df_submit = pd.DataFrame([], columns=['Id', 'Label'])
df_submit['Id'] = [f'{i:04d}' for i in range(len(x_test_normalize))]
df_submit['Label'] = np.argmax(model.predict(x_test_normalize), axis=-1)

df_submit.to_csv('submission.csv', index=None)